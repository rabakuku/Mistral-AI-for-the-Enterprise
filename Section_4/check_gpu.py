import torch
import sys
import os

def validate_enclave():
    print("="*50)
    print("ğŸ›¡ï¸  SOVEREIGN AI ENGINE: GPU VALIDATION")
    print("="*50)

    # 1. Check Python Version
    print(f"ğŸ Python Version: {sys.version.split()[0]}")

    # 2. Check Driver Communication
    print(f"ğŸ“¡ Driver Link: ", end="")
    driver_check = os.popen("nvidia-smi --query-gpu=driver_version --format=csv,noheader").read().strip()
    if driver_check:
        print(f"âœ… ACTIVE (Driver: {driver_check})")
    else:
        print("âŒ FAILED (No NVIDIA driver detected)")

    # 3. Check PyTorch & CUDA
    cuda_available = torch.cuda.is_available()
    print(f"ğŸ”¥ CUDA Available: {'âœ… YES' if cuda_available else 'âŒ NO'}")

    if cuda_available:
        # 4. Identify the GPU Model
        gpu_name = torch.cuda.get_device_name(0)
        print(f"ğŸ¯ Target GPU: {gpu_name}")

        # 5. Memory Statistics (The L4 has 24GB)
        total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"ğŸ“Š Total VRAM: {total_mem:.2f} GB")

        # 6. Simple Computation Test
        # We send a small 'tensor' to the GPU to see if it can actually do math.
        try:
            x = torch.tensor([1.0, 2.0]).to("cuda")
            print("âš¡ Computation Test: âœ… SUCCESS (GPU is processing math!)")
        except Exception as e:
            print(f"âš¡ Computation Test: âŒ FAILED ({str(e)})")
    
    print("="*50)

if __name__ == "__main__":
    validate_enclave()
